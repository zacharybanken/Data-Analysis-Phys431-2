{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Analysis for the H-D Spectrum Experiment\n",
    "\n",
    "## Peak-fit method\n",
    "\n",
    "This template uses a multiple peak-fitting calculation to obtain the peak locations.  It is more sophisticated than the centroid method, but shares some features.\n",
    "\n",
    "Use this template to carry out the analysis tasks for the HD spectrum experiment.  You may need to consult the documentation for different Python packages.  Also recommended: the [Whirlwind Tour of Python](https://jakevdp.github.io/WhirlwindTourOfPython/) and the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) both by Jake VanderPlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, import some packages\n",
    "\n",
    "This is a good idea at the beginning of your notebook to include the packages that you will need.  We will use the four shown below here.  A brief description:\n",
    "* `numpy` is the foundational package for Python numerical work. It extends and speeds up array operations beyond standard Python, and it includes almost all math functions that you would need for example `sqrt()` (square root) or `cos()` (cosine).  These would be written in code as `np.sqrt()` or `np.cos()`.\n",
    "* `scipy` is a huge collection of scientific data analysis functions, routines, physicical constants, etc.  This is the second most used package for scientific work. Documentation is at [SciPy.org](https://docs.scipy.org/doc/scipy/reference/) with the constants subpackage at https://docs.scipy.org/doc/scipy/reference/constants.html.\n",
    "* `uncertainties` is a very useful small package that simplifies uncertainty propagation and printing out of quantities with uncertainty.  Documentation is at https://pythonhosted.org/uncertainties/\n",
    "* `matplotlib` is *the* standard plotting package for scientific Python.  We will use a subset called `pyplot` which is modeled after the plotting functions used in MATLAB. The last line below, `%matplotlib inline`, simply forces the plots to appear within the notebook.\n",
    "* `pandas` is a large data science package.  It's main feature is a set of methods to create and manipulate \"Dataframes\", which is an enlargement of the idea of an array.  It plays well with NumPy and other packages.  We will use it mainly as a way to read files into data sets in an easy way.\n",
    "\n",
    "We will also be using the [**LMFit**](https://lmfit.github.io/lmfit-py/) curve fitting package.  This library is very powerful and relatively easy to use to perform complex fitting of functions to data sets, and to also extract meaningful values for statistical uncertainties in the fitting parameters.  It will be introduced and imported later in the template.  Documentation is at https://lmfit.github.io/lmfit-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell with Shift-Enter, and wait until the asterisk changes to a number, i.e., [*] becomes [1]\n",
    "import numpy as np\n",
    "import scipy.constants as const\n",
    "import uncertainties as unc\n",
    "import uncertainties.unumpy as up\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Calibration\n",
    "\n",
    "### Read in the data\n",
    "\n",
    "It is always a good idea to make a plot of the raw data before trying to manipulate it. Before doing this, you should open a data file with a simple text editor to see what it looks like.  You will see some information at the beginning about th oscilloscope's parameters.\n",
    "\n",
    "Then use the **Pandas** function `read_csv()` to pull the file into a Pandas Dataframe, like this:\n",
    "\n",
    "```\n",
    "Na = pd.read_csv('Na_D_lines.csv', skiprows=15)\n",
    "```\n",
    "\n",
    "The variable `skiprows` makes the function skip over the header information in the data file produced by the oscilloscope.\n",
    "\n",
    "If the last line in the cell is the name of the dataframe, it will print a nice table.\n",
    "\n",
    "You may obtain the arrays for each column by using the column label, e.g., `Na['CH1']` is the array of the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines allow the user to enter a data file\n",
    "# df = pd.read_csv(filename,skiprows=15)  # skiprows skips header information.\n",
    "\n",
    "# Collect all the data sets into active dataframes.\n",
    "Na = pd.read_csv('Na_D_lines_run1.csv',skiprows=15)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using the label alone causes the dataframe to be displayed.\n",
    "\n",
    "# The \"[:10]\" truncates the view to the first 10 rows.\n",
    "\n",
    "Na[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "Below, I show how. Study the commands, change them, and see what happens.  Hint: study the sections in the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) on Matplotlib.\n",
    "\n",
    "Note also the minus sign in front of `Na['CH1']` in the `plt.plot()` function below.  What happens if you remove it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.grid()\n",
    "plt.title('Na Calibration scan')\n",
    "plt.ylabel(r'Intensity (V)')\n",
    "plt.xlabel(r'Time (s)')\n",
    "plt.plot(Na['TIME'],-Na['CH1'],'-',label='Na D-lines')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the peak maxima\n",
    "\n",
    "There are two parts to this.  Part 1 is to smooth the data, part 2 is to locate the maxima of the smoothed data and the width of the peaks located under the maxima.\n",
    "\n",
    "To do this we will need two functions from SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import functions from SciPy's collection.  You only need to do an import once in a given session\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1.** First, you want to smooth the y-axis (voltage) data a little bit.  Look closely at the plot, and you will notics a kind of \"stairstep\" quality, especially near the baseline.  This is the limit of the digitization resolution. Smoothing the data a small amount will remove much of this and make it easier to see the shape of the curve.\n",
    "\n",
    "One way is to use \"Gaussian filtering\" to do this.  Gaussian filtering uses a gaussian to convolve with the data set.  The numerical parameter parameter gives the width of the gaussian; larger widths mean more smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing with a parameter of 5\n",
    "# This way of doing it adds another column to the dataframe.\n",
    "Na['CH1_smoothed'] = gaussian_filter1d(Na['CH1'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting the smoothed data over the original data.  Then change the `5` in the `gaussian_filter1d()` function to `100` and see what happens.  **Message:** Be careful with smoothing data sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.grid()\n",
    "plt.title('Na Calibration scan')\n",
    "plt.ylabel(r'Intensity (V)')\n",
    "plt.xlabel(r'Time (s)')\n",
    "plt.plot(Na['TIME'],-Na['CH1'],'-',label='Na D-lines')\n",
    "plt.plot(Na['TIME'],-Na['CH1_smoothed'],'-',label='Smoothed data')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.** Second, you can use a peak-finding function from SciPy.  This will locate the array indices of peaks in the data sets that satisfy certain criteria.  Using this function takes some care.  You should plot the peak locations and other parameters you are trying to find to make sure it is doing what you want.\n",
    "\n",
    "I show an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assign current arrays to xdata, ydata\n",
    "ydata = -Na['CH1_smoothed']\n",
    "xdata = Na['TIME']\n",
    "\n",
    "# A \"min width\" keeps small fluctuations near the top from being labeled separate peaks\n",
    "# Width units are array indices\n",
    "min_width = 50\n",
    "\n",
    "# Below does the work.  The height parameter makes the function only look \n",
    "# for peaks higher than halfway up the tallest peak.\n",
    "peaks, pk_props = find_peaks(ydata, width = min_width, height = ydata.max()/2.)\n",
    "\n",
    "for pk, prop in zip(peaks, pk_props['widths']): \n",
    "    print('Peak at {:d} has width {:.1f}'.format(pk, prop))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show this with a plot.  Note the use of vlines() and hlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.grid()\n",
    "plt.title('Na Calibration scan')\n",
    "plt.ylabel(r'Intensity (V)')\n",
    "plt.xlabel(r'Array index')\n",
    "plt.plot(ydata,'-',label='Smoothed data')\n",
    "plt.vlines(x=peaks, ymin=0, ymax=ydata[peaks], color='C2', label='Locations')\n",
    "plt.hlines(y=pk_props['width_heights'], xmin=peaks-pk_props['widths']/2, \n",
    "           xmax=peaks+pk_props['widths']/2, color = 'C1', label='Widths')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now save the peak positions and widths.  These will be used to determine starting values for a peak-fitting routine.\n",
    "\n",
    "The peak positions and widths are in units of array index.  One needs to convert to the units of the `xdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpeaks = np.array([xdata[peaks[0]], xdata[peaks[1]]])\n",
    "xpeaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2time = (xdata[peaks[1]]-xdata[peaks[0]])/(peaks[1]-peaks[0])\n",
    "xwidths = pk_props['widths']*idx2time\n",
    "xwidths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the peaks\n",
    "\n",
    "To fit the data to peaks, make use of the **lmfit** package. It is a useful add-on to the SciPy fitting functions.  This package simplifies fitting data to a variety of standard functions.  See the [Lmfit Documentation](https://lmfit.github.io/lmfit-py/index.html) for a full discussion.  The package is quite powerful, but for basic fitting with common functions, it is very easy to use.  \n",
    "\n",
    "#### Example: Fitting a line\n",
    "\n",
    "We'll start with an example fit to a line.  You will use a line fit later in the notebook, but it is simpler than a peak fit, so it makes a good beginning example.\n",
    "\n",
    "The example shows how to fit data to a line, obtain the fit parameters along with uncertainties, and then plot the data and fit. Execute the cells and study how it works. \n",
    "\n",
    "The first cell is just example data. (They come from a calibration problem in physics 331, optics lab)  You will modify the function calls when you use it on your own results later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Data\n",
    "# First column is wavelength (nm), second is carriage poisition (cm)\n",
    "#\n",
    "Cal_data = np.array([\n",
    "    [643.85, 41.43],\n",
    "    [579.07, 37.24],\n",
    "    [576.96, 37.11],\n",
    "    [546.08, 35.10],\n",
    "    [508.58, 32.68],\n",
    "    [479.99, 30.83],\n",
    "    [467.81, 30.04],\n",
    "    [435.83, 27.96],\n",
    "    [404.66, 25.98]])\n",
    "\n",
    "# Array slicing separates x (position) and y (wavelength)\n",
    "# Goal of calibration is to be able to feed in a position and obtain a wavelength\n",
    "wavelength = Cal_data[:,0]\n",
    "position = Cal_data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the fitting example.  Note the very first command to import a fitting \"Model\" called `LinearModel`.\n",
    "\n",
    "A \"Model\" is an LMFit object that contains information about the fitting function and methods to help set up and manipulate the fitting parameters.  Notice what each line does, and how the return value of the function call is used in subsequent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports a linear fitting model from lmfit\n",
    "# You only need to execute this line once!\n",
    "from lmfit.models import LinearModel\n",
    "\n",
    "# create an instance of the model\n",
    "line = LinearModel()\n",
    "\n",
    "# One must have a guess of the parameters. The guess() method works with most of the standard\n",
    "# lmfit models\n",
    "param_guess = line.guess(wavelength, x=position)\n",
    "\n",
    "# The line below executes the fitting process.  The results are returned to \"line_fit\"\n",
    "line_fit = line.fit(wavelength, param_guess, x=position)\n",
    "\n",
    "# This prints the results in an easy to read form\n",
    "print(line_fit.fit_report())\n",
    "\n",
    "# The parameters and uncertainties are accessible as follows, for example:\n",
    "print('\\nSlope = ',line_fit.params['slope'].value,'+/-',line_fit.params['slope'].stderr)\n",
    "\n",
    "#Then you can plot the results quickly just to see how it looks using the plot() method\n",
    "line_fit.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the peak-fitting model\n",
    "\n",
    "Now we expand the example to a more complicated problem: a fit of two Gaussian peaks and a constant background.\n",
    "\n",
    "The Gaussian peak has the form\n",
    "$$f(x; A,\\mu,\\sigma) = \\frac{A}{\\sigma \\sqrt{2\\pi}}e^{[(x-\\mu)^2/2\\sigma^2]}\\;.$$ \n",
    "The parameters are `amplitude`, `center`, and `sigma`. In addition, parameters `fwhm` and `height` are included as constraints to report full width at half maximum and maximum peak height, respectively.\n",
    "\n",
    "Note that the `height` of the peak at the center ($x=\\mu$) is equal to $\\frac{A}{\\sigma\\sqrt{2\\pi}}$ and that the full-width at half-maximum is about 2.3548$\\sigma$.\n",
    "\n",
    "Thus, to estimate the amplitude $A$, estimate the height $h$ and FWHM from a graph, and calculate $A = h*$FWHM.\n",
    "\n",
    "The contant background sets the height of the data set \"floor.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a gaussian peak and second order polynomial for background\n",
    "\n",
    "from lmfit.models import ConstantModel, GaussianModel\n",
    "\n",
    "# create an instance of the model\n",
    "# Note the use of prefixes\n",
    "model = ConstantModel() + GaussianModel(prefix='p1_') + GaussianModel(prefix='p2_')\n",
    "\n",
    "# Necessary step below.  This replaces the guess() method used earlier.  The guess() method \n",
    "# does not work for composite models.\n",
    "params = model.make_params()\n",
    "\n",
    "# Notice how the prefixes are attached to the parameter names.\n",
    "print('parameter names: {}'.format(model.param_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameters\n",
    "\n",
    "For a composite fit, when you cannot use `guess()`, you need to set the values of the parameters by hand. Information has already been found to do this from the above peak-finding steps.\n",
    "\n",
    "Study the lines below, and note how the values of each parameter is set.\n",
    "\n",
    "Also notice the use of the `vary` property for each fitting parameter.  If you set `vary =  False`, then that parameter will not be adjusted during the fit.  \n",
    "\n",
    "This is one way to make a line fit go through zero. For example:\n",
    "\n",
    "```param_guess['intercept'].set(value=0, vary=False)``` \n",
    "\n",
    "placed before the call `line_fit = line.fit(wavelength, param_guess, x=position)` in the line fit example above would force the line through zero and find the best fit slope under that constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak 1 starting values\n",
    "\n",
    "p1_loc = xpeaks[0]\n",
    "p1_sig = xwidths[0]/2.3548\n",
    "p1_amp = ydata[peaks[0]]*xwidths[0]\n",
    "\n",
    "# Peak 2 starting values\n",
    "\n",
    "p2_loc = xpeaks[1]\n",
    "p2_sig = xwidths[1]/2.3548\n",
    "p2_amp = ydata[peaks[1]]*xwidths[0]\n",
    "\n",
    "# Build the parameters\n",
    "\n",
    "params['p1_center'].set(value=p1_loc, vary=True)\n",
    "params['p1_amplitude'].set(value=p1_amp, vary=True)\n",
    "params['p1_sigma'].set(value=p1_sig, vary=True)\n",
    "\n",
    "params['p2_center'].set(value=p2_loc, vary=True)\n",
    "params['p2_amplitude'].set(value=p2_amp, vary=True)\n",
    "params['p2_sigma'].set(value=p2_sig, vary=True)\n",
    "\n",
    "params['c'].set(value = 0.01, vary=True)\n",
    "\n",
    "params.pretty_print(columns=['value','vary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the parameters, the rest of the fitting process is the same as for a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(ydata, params, x=xdata)\n",
    "\n",
    "print(model_fit.fit_report(show_correl=False))\n",
    "\n",
    "myfig=plt.figure(figsize=(10,10))\n",
    "model_fit.plot(fig=myfig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the peak locations and uncertainty into uncertainty objects `ufloat`.  These are in the parameters `p1_center` and `p2_center`.  Notice how the code below searches out the `_center` parameters using a string search function.\n",
    "\n",
    "Calculations with uncertainty objects will automatically propagate the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Peak_loc = []\n",
    "\n",
    "for parm in model_fit.params:\n",
    "    if parm.endswith('center'):\n",
    "        # make an uncertainty object\n",
    "        this_center = unc.ufloat(model_fit.params[parm].value, model_fit.params[parm].stderr)\n",
    "        # add it to the Python list\n",
    "        Peak_loc.append(this_center)\n",
    "        # also print it\n",
    "        print('Peak at {:.2uP} s'.format(this_center))\n",
    "\n",
    "# Save the list to a new variable \n",
    "Na_loc = Peak_loc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the calibration constant with its uncertainty.  This is easy if you are using the uncertainty object.\n",
    "\n",
    "The calibration constant $K_\\text{Na}$ is simply\n",
    "\n",
    "$$ K_\\text{Na} = \\frac{\\lambda_{\\text{D}_1}-\\lambda_{\\text{D}_2}}{t_{\\text{D}_1}-t_{\\text{D}_2}}$$\n",
    "\n",
    "By definition, $D_1$ has a longer wavelength than $D_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 5895.92 # Angstroms\n",
    "D2 = 5889.95 # Angstroms\n",
    "\n",
    "Na_cal = (D1-D2)/(Na_loc[1]-Na_loc[0])\n",
    "\n",
    "print('Calibration constant from peak-fit method: {:.2uP} A/s'.format(Na_cal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data reduction\n",
    "\n",
    "### Read in and plot all of the data\n",
    "\n",
    "Before doing any calculations, you shoul always plot and look hard at your data.\n",
    "\n",
    "Repeat the steps at the beginning to feed each data set into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect all the data sets into active dataframes. \n",
    "## Follow the method used for the Na lines\n",
    "\n",
    "## Recommended names  for te dataframes 'Alpha', 'Beta', 'Gamma', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot all on one graph.  Note use of \"-\" to flip all data sets to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(Na['TIME'],-Na['CH1'],'-',label='Na D-lines')\n",
    "plt.plot(Alpha['TIME'],-Alpha['CH1'],'-',label=r'$\\alpha$')\n",
    "\n",
    "# ....\n",
    "# add the rest here\n",
    "\n",
    "plt.xlabel(r'Time (s)')\n",
    "plt.ylabel(r'Ch 1 (V)')\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to read.  Let's offset the plots and rescale them.  Note use of multiplication factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut and paste above, and then manipulate arrays within the plot() calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each data file\n",
    "\n",
    "At this point, you want to apply the same basic steps to each H-D spectrum files, namely:\n",
    "\n",
    "* Smooth it.\n",
    "* Locate the peak positions and widths.\n",
    "* Use this information to feed the starting values for the fitting model.\n",
    "* Extract the peak centers and uncertainty from the fit results.\n",
    "* Calculate $\\Delta t_\\text{HD}$ for that data set, and save it.\n",
    "\n",
    "Before doing these steps, convert the code blocks used to carry out these tasks above to functions.  This will make your code reuse less messy and easier to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to locate the peaks, plot them, and return the important data\n",
    "\n",
    "def find_and_plot_peaks(ydata, title='Data', makeplot=True):\n",
    "    '''\n",
    "    Function locates peaks using SciPy.signal find_peaks() and plots\n",
    "    the results (optionally).  Returns two arrays: 'peaks' which holds\n",
    "    the index of each peak maximum and 'widths' which holds the FWHM of\n",
    "    each peak.\n",
    "    '''\n",
    "    # You fill this in here  \n",
    "        \n",
    "    return peaks, pk_props['widths']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to obtain starting values for fit\n",
    "def guess_start_parms(xdata, ydata, peaks, widths, params):\n",
    "    '''\n",
    "    Calculates starting values for the peak fitting routine.  \n",
    "    Returns params object\n",
    "    '''\n",
    "    # You fill in the rest\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_peak_locs(xdata, ydata, params):\n",
    "    '''\n",
    "    Fit data to two gaussian peaks and extract peak positions.\n",
    "    Plots result and extracts centers with uncertainty.\n",
    "    Returns a list of uncertainty objects\n",
    "    '''\n",
    "    \n",
    "    return Peak_loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the above.  Recommend commenting out later lines and examining the return values at each step\n",
    "\n",
    "Na_pks, Na_widths = find_and_plot_peaks(ydata, title='Na Lines', makeplot=True)\n",
    "Na_start_params = guess_start_parms(xdata, ydata, Na_pks, Na_widths, params)\n",
    "Na_test_loc = calculate_peak_locs(xdata, ydata, Na_start_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the functions\n",
    "\n",
    "Work on Alpha first. I show this as an example.  Reuse `xdata`, `ydata`, `peaks`, `widths` and `start_params` to save typing.  **But be careful:** if you execute cells out of sequence, you will get peculiar results!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha['CH1_smoothed'] = gaussian_filter1d(Alpha['CH1'], 5)\n",
    "\n",
    "xdata = Alpha['TIME']\n",
    "ydata = -Alpha['CH1_smoothed']\n",
    "\n",
    "peaks, widths = find_and_plot_peaks(ydata, title='Alpha Line', makeplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: Fit and grab peaks.\n",
    "\n",
    "Results saved to `Alpha_loc`.  Will repeat for `Beta_loc`, `Gamma_loc`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_params = guess_start_parms(xdata, ydata, peaks, widths, params)\n",
    "\n",
    "Alpha_loc = calculate_peak_locs(xdata, ydata, start_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for other lines.  \n",
    "\n",
    "Beta next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta['CH1_smoothed'] = gaussian_filter1d(Beta['CH1'], 5)\n",
    "\n",
    "xdata = Beta['TIME']\n",
    "ydata = -Beta['CH1_smoothed']\n",
    "\n",
    "## Etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second and third calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You what to do now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Epsilon lines (2 sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the time differences\n",
    "\n",
    "One easy way is to make a simple Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_T = [Alpha_loc[1]-Alpha_loc[0],\n",
    "      Beta_loc[1]-Beta_loc[0],\n",
    "      Gamma_loc[1]-Gamma_loc[0],\n",
    "      Delta_loc[1]-Delta_loc[0],\n",
    "      Eps1_loc[1]-Eps1_loc[0],\n",
    "      Eps2_loc[1]-Eps2_loc[0]]\n",
    "D_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fancier way is to build a Pandas Series.  I will also average the two epsilon runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta_T = pd.Series({'Alpha': Alpha_loc[1]-Alpha_loc[0],\n",
    "                   'Beta': Beta_loc[1]-Beta_loc[0],\n",
    "                   'Gamma': Gamma_loc[1]-Gamma_loc[0],\n",
    "                   'Delta': Delta_loc[1]-Delta_loc[0],\n",
    "                   'Epsilon': (Eps1_loc[1]+Eps2_loc[1]-Eps1_loc[0]-Eps2_loc[0])/2.0})\n",
    "Delta_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then include the known hydrogen wavelengths $\\lambda_\\text{H}$.  A table can be found in Haken & Wolf, p.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_H = pd.Series({'Alpha': 6562.79,\n",
    "                   'Beta': 4863.33,\n",
    "                   'Gamma': 4340.46,\n",
    "                   'Delta': 4101.73,\n",
    "                   'Epsilon': 3970.07})\n",
    "lambda_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HD_results = pd.DataFrame({'lambda_H (A)':lambda_H, 'Delta-t (s)': Delta_T})\n",
    "HD_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HD_results['Delta-lambda (A)'] = HD_results['Delta-t (s)']*Na_cal\n",
    "HD_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a plot\n",
    "\n",
    "Plot the results of $\\Delta t_\\text{HD}$ versus $\\lambda_\\text{H}$ with the uncertainty as errorbars.  Include the origin in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.grid()\n",
    "plt.title('HD spectrum results')\n",
    "plt.ylim(0,9.8)\n",
    "plt.xlim(0,6800.0)\n",
    "plt.ylabel(r'Time difference between H,D peaks $\\Delta t_{\\rm HD}$ (s)')\n",
    "plt.xlabel(r'Hydrogen wavelength (A)')\n",
    "plt.errorbar(HD_results['lambda_H (A)'],up.nominal_values(HD_results['Delta-t (s)']),fmt='o',\n",
    "             yerr=up.std_devs(HD_results['Delta-t (s)']),label='Spectrum data');\n",
    "plt.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit to a line\n",
    "\n",
    "Fit these results to a line that goes through zero.  Go back to the line fitting example, and copy the relevant code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put the data into simple arrays: xdata, ydata, yuncert\n",
    "\n",
    "xdata = HD_results['lambda_H (A)']\n",
    "ydata = up.nominal_values(HD_results['Delta-t (s)'])\n",
    "yuncert = up.std_devs(HD_results['Delta-t (s)'])\n",
    "\n",
    "## Then use the earlier code starting with \"param_guess = line.guess(wavelength, x=position)\"\n",
    "## and change the variable assignments appropriately to make your fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a nice plot\n",
    "\n",
    "Replot the above data-plot but include the fitline so that you can see the line go through zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code below makes a fitline\n",
    "\n",
    "xfit = np.linspace(0,6600,10)\n",
    "yfit = line_fit.eval(line_fit.params, x=xfit)\n",
    "\n",
    "## Cut and paste the earlier plot, and include the fitline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the slope from the fit.\n",
    "HD_slope = unc.ufloat(line_fit.params['slope'].value,line_fit.params['slope'].stderr)\n",
    "\n",
    "## Then use it in a calculation to obtain the mass ratio and uncertainty, and print it out\n",
    "\n",
    "## You write code to calculate the mass ratio.\n",
    "\n",
    "print('Calculated Mass ratio of hydrogen/deuterium = {:.1uP}'.format(mass_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
